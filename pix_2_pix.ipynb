{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **generator** architecture in *Pix2Pix* uses a *U-Net* structure. *U-Net* has in a *encoder-decoder* network famous by its *skip connections* that help preserve fine details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetEncoder(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_features=64):\n",
    "        super(UNetEncoder, self).__init__()\n",
    "        # This module lets PyTorch recognize and manage the layers correctly during training.\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Each step in the encoder halves the spatial dimensions of the input. \n",
    "        # Due to this downsampling, we can increase the number of filters learned by the model at each step. \n",
    "        # This is why the number of features is doubled at each step.\n",
    "\n",
    "        # Batch normalization is not used in the first layer of the encoder. Applying BatchNorm immediately after\n",
    "        # the first convolution could distort the input distribution, making training unstable.\n",
    "        self.layers.append(self._create_down_block(in_channels, num_features, batch_norm=False)) # 256x256x3 -> 128x128x64\n",
    "\n",
    "        self.layers.append(self._create_down_block(num_features, num_features*2)) # 128x128x64 -> 64x64x128\n",
    "        self.layers.append(self._create_down_block(num_features*2, num_features*4)) # 64x64x128 -> 32x32x256\n",
    "        self.layers.append(self._create_down_block(num_features*4, num_features*8)) # 32x32x256 -> 16x16x512\n",
    "\n",
    "        for _ in range(3):\n",
    "            self.layers.append(self._create_down_block(num_features*8, num_features*8)) \n",
    "        # The output shape after these 3 blocks is 2x2x512.\n",
    "\n",
    "        # In deeper layers, feature maps become low-resolution. When this happens, we have a limited statistical diversity, \n",
    "        # leading to poor mean/variance estimates. Because of that, we avoid the use o batch normalization.\n",
    "        # Dropout randomly drops a fraction of neurons during training to prevent over-reliance on certain features.\n",
    "        self.layers.append(self._create_down_block(num_features*8, num_features*8, dropout=0.5, batch_norm=False)) # 2x2x512 -> 1x1x512\n",
    "    \n",
    "    # The down block consists of a convolutional layer, followed by batch normalization, Leaky ReLU activation, and dropout.\n",
    "    def _create_down_block(self, input_channels, out_channels, batch_norm=True, dropout=0.0):\n",
    "        return nn.Sequential(\n",
    "            # The stride of 2 halves the spatial dimensions of the input. Also, padding is used to maintain the spatial dimensions.\n",
    "            nn.Conv2d(input_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_channels) if batch_norm else nn.Identity(),\n",
    "            # We use Leaky ReLU activation function to avoid the dying ReLU problem.\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(dropout) if dropout > 0 else nn.Identity(),\n",
    "        )\n",
    "    \n",
    "    # The forward method iterates over the layers and applies them sequentially. \n",
    "    def forward(self, x):\n",
    "        # During the forward pass, we store the feature maps of each layer in the skips list.\n",
    "        # This list is used in the decoder to concatenate the feature maps of the encoder with the feature maps of the decoder.\n",
    "        skips = []\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(x)\n",
    "            skips.append(x)\n",
    "\n",
    "        # The bottleneck of the encoder does not have a skip connection.\n",
    "        x = self.layers[-1](x)\n",
    "        return x, skips\n",
    "    \n",
    "\n",
    "# The decoder is the second half of the U-Net architecture. It consists of up-sampling blocks to reconstruct the image\n",
    "# from the extracted features.\n",
    "class UNetDecoder(nn.Module):\n",
    "    def __init__(self, out_channels=3, num_features=64):\n",
    "        super(UNetDecoder, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # The first block receives only the output of the encoder. There is no skip connection here yet.\n",
    "        # This layer also uses dropout to prevent overfitting.\n",
    "        self.layers.append(self._create_up_block(num_features*8, num_features*8, dropout=0.5)) # 1x1x512 -> 2x2x512\n",
    "\n",
    "        # Here we start concatenating the feature maps of the encoder with the output of the previous layer. Because\n",
    "        # of this, the number of input channels is doubled.\n",
    "        for _ in range(2):\n",
    "            self.layers.append(self._create_up_block(num_features*16, num_features*8, dropout=0.5)) # 2x2x1024 -> 4x4x512\n",
    "                                                                                                    # 4x4x1024 -> 8x8x512\n",
    "        \n",
    "        # The following blocks are similar to the previous ones, but the number of input channels is halved at each step. \n",
    "        # Also, we don't use dropout since the resolution is higher, thus less propense to overfitting.\n",
    "        self.layers.append(self._create_up_block(num_features*16, num_features*8)) # 8x8x1024 -> 16x16x512\n",
    "        self.layers.append(self._create_up_block(num_features*16, num_features*4)) # 16x16x1024 -> 32x32x256\n",
    "        self.layers.append(self._create_up_block(num_features*8, num_features*2)) # 32x32x512 -> 64x64x128\n",
    "        self.layers.append(self._create_up_block(num_features*4, num_features)) # 64x64x256 -> 128x128x64\n",
    "\n",
    "        # The last layer of the decoder is a convolutional layer that reduces the number of channels to the desired output.\n",
    "        # Batch normalization is disabled to avoid distorting the output distribution.\n",
    "        self.layers.append(self._create_up_block(num_features*2, out_channels, batch_norm=False)) # 128x128x128 -> 256x256x3\n",
    "\n",
    "    # The up block consists of a transposed convolutional layer, followed by batch normalization, ReLU activation, and dropout.\n",
    "    # This block recover the spatial dimensions of the input.\n",
    "    def _create_up_block(self, input_channels, out_channels, batch_norm=True, dropout=0.0):\n",
    "        return nn.Sequential(\n",
    "            # We use the same kernel size and stride as the encoder.\n",
    "            nn.ConvTranspose2d(input_channels, out_channels, kernel_size=3, stride=2, padding=1, bias=not batch_norm),\n",
    "            nn.BatchNorm2d(out_channels) if batch_norm else nn.Identity(),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout) if dropout else nn.Identity(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, skips):\n",
    "        skips = list(reversed(skips))\n",
    "        x = self.layers[0](x) # The first layer processes only the bottleneck output.\n",
    "\n",
    "        # The following layers concatenate the feature maps of the encoder with the feature maps\n",
    "        # of the decoder through the skip connections.\n",
    "        for i, layer in enumerate(self.layers[1:]):\n",
    "            x = torch.cat((x, skips[i]), dim=1)\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_features=64):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # THe generator is simply the encoder and decoder combined.\n",
    "        # In total, we have 8 down blocks and 8 up blocks.\n",
    "        self.encoder = UNetEncoder(in_channels, num_features)\n",
    "        self.decoder = UNetDecoder(out_channels, num_features)\n",
    "    \n",
    "    # The forward method of the generator first passes the input through the encoder.\n",
    "    # The output of the encoder and the skip connections are then passed to the decoder.\n",
    "    def forward(self, x):\n",
    "        x, skips = self.encoder(x)\n",
    "        x = self.decoder(x, skips)\n",
    "        x = torch.tanh(x) # Normalize to [-1, 1]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **discriminator** uses a *PatchGAN* architecture to classify image patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Pix2Pix, the discriminator is a PatchGAN, which means it classifies overlapping patches of the image as real or fake. \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels, num_features=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # The discriminator uses a series of convolutional layers with increasing filters, reducing spatial dimensions each time.\n",
    "        # The final layer outputs a matrix where each element corresponds to a patch of the input image.\n",
    "        self.model = nn.Sequential(\n",
    "            # Considering a 256x256 image, the first layer outputs a 128x128 matrix.\n",
    "            self._create_block(in_channels*2, num_features, stride=2, batch_norm=False), # 256x256x6 -> 128x128x64\n",
    "            self._create_block(num_features, num_features*2, stride=2), # 128x128x64 -> 64x64x128\n",
    "            self._create_block(num_features*2, num_features*4, stride=2), # 64x64x128 -> 32x32x256\n",
    "            self._create_block(num_features*4, num_features*8, stride=1), # 32x32x256 -> 31x31x512\n",
    "            nn.Conv2d(num_features*8, 1, kernel_size=4, stride=1, padding=1), # 31x31x512 -> 30x30x1\n",
    "        )\n",
    "        \n",
    "    def _create_block(self, in_channels, out_channels, stride, batch_norm=True):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels) if batch_norm else nn.Identity(),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "\n",
    "    # in the Pix2Pix GAN, the discriminator receives the input image and the target image as input.\n",
    "    # Because of that, it is called a conditional GAN, because the discriminator is conditioned on the target image.\n",
    "    def forward(self, x, y):\n",
    "        # The input image and the target image are concatenated along the channel dimension.\n",
    "        return self.model(torch.cat([x, y], dim=1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "generator = Generator(3, 3).to(device)\n",
    "discriminator = Discriminator(3).to(device)\n",
    "\n",
    "# Loss functions.\n",
    "\n",
    "# Adversarial loss (Binary Cross-Entropy loss).\n",
    "adv_criterion = nn.BCEWithLogitsLoss()\n",
    "# L1 loss (for generator to preserve structure).\n",
    "l1_criterion = nn.L1Loss()\n",
    "\n",
    "# Optimizers.\n",
    "\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "lambda_l1 = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Custom dataset should return pairs (input, target)\n",
    "train_dataset = YourPairedDataset(transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    for real_A, real_B in dataloader:\n",
    "        real_A, real_B = real_A.to(device), real_B.to(device)\n",
    "        \n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        fake_B = generator(real_A)\n",
    "        D_real = discriminator(real_A, real_B)\n",
    "        D_fake = discriminator(real_A, fake_B.detach())\n",
    "        \n",
    "        real_loss = adv_criterion(D_real, torch.ones_like(D_real))\n",
    "        fake_loss = adv_criterion(D_fake, torch.zeros_like(D_fake))\n",
    "        D_loss = (real_loss + fake_loss) * 0.5\n",
    "        D_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "        fake_B = generator(real_A)\n",
    "        D_fake = discriminator(real_A, fake_B)\n",
    "        \n",
    "        G_adv_loss = adv_criterion(D_fake, torch.ones_like(D_fake))\n",
    "        G_l1_loss = l1_criterion(fake_B, real_B) * lambda_l1\n",
    "        G_loss = G_adv_loss + G_l1_loss\n",
    "        G_loss.backward()\n",
    "        optimizer_G.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
